{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element resolution from an arbitrary spectra is likely going to be a multivariate fitting so this notebook explores available optimization libraries in python. \n",
    "Advancing LIBS to calibration free estimation may involve allowing for variation in plasma conditions by element and time. For example, if certain ions are emitting at differnt points then the plasma temp/density can vary across the lines observed. In fitting the observed data we may be able to estimate these parameters from relative line strengths observed. Alternatively we could allow these parameters to vary to determine optimal fit. The implied sample elemental composition will be better fit.\n",
    "\n",
    "Breaking each element down into ionization states and the associated energies/probabilities as f(Temp,dens) each could be fit separately and normalized to a reference Temp/Dens upon which full elemental composition could be estimated.\n",
    "\n",
    "SciPy is a good starting point for optimization routines, though there are additional python libraries available to review.\n",
    "https://docs.scipy.org/doc/scipy/tutorial/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective function must have signature f(x, *args) where x is numpy array\n",
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 339\n",
      "         Function evaluations: 571\n"
     ]
    }
   ],
   "source": [
    "#nelder-mead is a simplex algorithm and does not use gradient (Powell also needs only function calls)\n",
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = minimize(rosen, x0, method='nelder-mead',\n",
    "               options={'xatol': 1e-8, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 25\n",
      "         Function evaluations: 180\n",
      "         Gradient evaluations: 30\n"
     ]
    }
   ],
   "source": [
    "#Faster convergence from methods that include a gradient, Can be passed or estimated from first diffs\n",
    "#Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')\n",
    "res = minimize(rosen, x0, method='BFGS', options={'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include the gradient explicitly, best in the objective def which now returns a tuple\n",
    "def rosen_and_der(x):\n",
    "    objective = sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return objective, der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 25\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n"
     ]
    }
   ],
   "source": [
    "#note if sep fxn for jacobian, specify that as jac arg\n",
    "res = minimize(rosen_and_der, x0, method='BFGS', jac=True, options={'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standalond jacobian for next method call\n",
    "def rosen_der(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton-conjugate takes second derivative into account via Hessian matrix\n",
    "#Basically a two-term Taylor expansion\n",
    "def rosen_hess(x):\n",
    "    x = np.asarray(x)\n",
    "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "    diagonal = np.zeros_like(x)\n",
    "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "    diagonal[-1] = 200\n",
    "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "    H = H + np.diag(diagonal)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 24\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "         Hessian evaluations: 24\n"
     ]
    }
   ],
   "source": [
    "#note, another form allows for hessp= which is function giving product of Hessian with arb vector p\n",
    "res = minimize(rosen, x0, method='Newton-CG',\n",
    "                jac=rosen_der, hess=rosen_hess,\n",
    "                options={'xtol': 1e-8, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trust-Region Newton-Conjugate-Gradient Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
